import streamlit as st
import os
import time
import google.generativeai as genai
from google.generativeai import caching
import datetime

# 1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="2025ìƒí™œê¸°ë¡ë¶€ ë§¤ë‰´ì–¼", page_icon="ğŸ“š")

# 2. ì‚¬ì´ë“œë°”: ì„¤ì • ë° ì±… ì„ íƒ
with st.sidebar:
    st.header("ì„¤ì •")
    google_api_key = st.text_input("Google API Key", type="password")
    
    st.divider()
    st.subheader("ğŸ“š ì„œë²„ì— ì €ì¥ëœ ì±… ëª©ë¡")
    
    current_dir = os.getcwd()
    pdf_files = [f for f in os.listdir(current_dir) if f.endswith('.pdf')]
    
    if not pdf_files:
        st.error("âš ï¸ ì„œë²„ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
        st.info("ê¹ƒí—ˆë¸Œ ë¦¬í¬ì§€í† ë¦¬ì— .pdf íŒŒì¼ì„ í•¨ê»˜ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        selected_file = None
    else:
        selected_file = st.selectbox("ì½ì„ ì±…ì„ ì„ íƒí•˜ì„¸ìš”", pdf_files)
        st.success(f"ì„ íƒë¨: {selected_file}")

# 3. ë©”ì¸ í™”ë©´
st.title("ğŸ“– 2025ìƒí™œê¸°ë¡ë¶€ ë§¤ë‰´ì–¼")
st.caption("Google Context Caching ê¸°ìˆ ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

if not google_api_key:
    st.warning("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ Google API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop()

if not selected_file:
    st.stop()

# API í‚¤ ì„¤ì •
genai.configure(api_key=google_api_key)

# 4. ìºì‹± ë¡œì§
if "cache_name" not in st.session_state:
    st.session_state.cache_name = None
if "current_book" not in st.session_state:
    st.session_state.current_book = ""

# ì±…ì´ ë³€ê²½ë˜ì—ˆê±°ë‚˜ ìºì‹œê°€ ì—†ìœ¼ë©´ ìƒì„± ì‹œì‘
if selected_file != st.session_state.current_book or st.session_state.cache_name is None:
    with st.spinner(f"ğŸš€ '{selected_file}' ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ êµ¬ê¸€ ì„œë²„ì— ì €ì¥ ì¤‘ì…ë‹ˆë‹¤... (ìµœì´ˆ 1íšŒ)"):
        try:
            # (1) íŒŒì¼ ê²½ë¡œ í™•ì¸
            file_path = os.path.join(current_dir, selected_file)
            
            # (2) êµ¬ê¸€ì— íŒŒì¼ ì—…ë¡œë“œ
            uploaded_file = genai.upload_file(file_path)
            print(f"ì—…ë¡œë“œ ì‹œì‘: {uploaded_file.name}") # ë¡œê·¸ í™•ì¸ìš©

            # (3) ì²˜ë¦¬ ëŒ€ê¸° (Processing State í™•ì¸)
            while uploaded_file.state.name == "PROCESSING":
                time.sleep(2) # 1ì´ˆëŠ” ë„ˆë¬´ ì§§ì„ ìˆ˜ ìˆì–´ 2ì´ˆë¡œ ë³€ê²½
                uploaded_file = genai.get_file(uploaded_file.name)
            
            if uploaded_file.state.name == "FAILED":
                raise ValueError("êµ¬ê¸€ ì„œë²„ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨")

            # (4) ìºì‹œ ìƒì„±
            # [ìˆ˜ì •ë¨] ëª¨ë¸ ì´ë¦„ì„ ì •í™•í•œ ë²„ì „ìœ¼ë¡œ ë³€ê²½ (gemini-1.5-flash-001)
            cache = caching.CachedContent.create(
                model='models/gemini-2.5-flash', 
                display_name=selected_file,
                system_instruction=(
                    "ë„ˆëŠ” 2025ë…„ ìƒí™œê¸°ë¡ë¶€ ì‘ì„±ì§€ì¹¨ì„ ì™„ì „í•˜ê²Œ ì•Œê³  ìˆëŠ” ì „ë¬¸ê°€ì•¼. ì§ˆë¬¸ìì˜ ê³ ë¯¼ì„ ë“£ê³  ì±…ì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒë‹´í•´ì¤˜. "
                    "ì±… ì „ì²´ ë‚´ìš©ì„ ë‹¤ ì•Œê³  ìˆìœ¼ë‹ˆ, ì†Œì œëª©ì´ë‚˜ ì „ì²´ì ì¸ ë§¥ë½ë„ ë‹¤ íŒŒì•…í•´ì„œ ë‹µë³€í•´. "
                    "ì±…ì— ì—†ëŠ” ë‚´ìš©ì€ ì§€ì–´ë‚´ì§€ ë§ê³ , ì±… ë‚´ìš©ì„ ì¸ìš©í•´ì„œ ë”°ëœ»í•˜ê²Œ ë§í•´ì¤˜."
                    "ë‚´ìš©ì— ì•Œë§ì€ ê·€ì—¬ìš´ ì´ëª¨í‹°ì½˜ë„ ë„£ì–´ê°€ë©´ì„œ ë‹µë³€í•´ì¤˜."
                    "í‘œë¡œ ë‹µë³€ì„ í•´ì•¼í•˜ëŠ” ê²½ìš°ëŠ” ì•„ë˜ì˜ ê·œì¹™ì„ ë°˜ë“œì‹œ ì§€ì¼œì„œ ì‘ì„±í•´ì¤˜."
                    "[í‘œ ì‘ì„± ê·œì¹™] 1.í‘œë¥¼ ì¶œë ¥í•  ë•ŒëŠ” ë°˜ë“œì‹œ í‘œì¤€ Markdown ë¬¸ë²•ì„ ì¤€ìˆ˜í•˜ì„¸ìš”."
                    "[í‘œ ì‘ì„± ê·œì¹™] 2.ì‹œê°ì ì¸ ì ì„ (------------)ì´ë‚˜ ì¥ì‹ìš© ì„ ì„ ì‚¬ìš©í•˜ì—¬ í‘œë¥¼ ê·¸ë¦¬ì§€ ë§ˆì„¸ìš”."
                    "[í‘œ ì‘ì„± ê·œì¹™] 3.ë°ì´í„°ê°€ ì—†ëŠ” ë¹ˆ í–‰ì„ ì ì„ ìœ¼ë¡œ ì±„ìš°ì§€ ë§ˆì„¸ìš”."
                    "[í‘œ ì‘ì„± ê·œì¹™] 4. ë°˜ë“œì‹œ | í—¤ë” | í—¤ë” | í˜•ì‹ ë°”ë¡œ ì•„ë˜ì— |---|---| í˜•ì‹ì„ ì‚¬ìš©í•˜ì—¬ í‘œë¥¼ ë Œë”ë§í•˜ì„¸ìš”."
                    # [ì¤‘ìš”] ê°œì¸ì •ë³´ ì˜¤ì¸ ë°©ì§€ í”„ë¡¬í”„íŠ¸ ìœ ì§€
                    "ì´ ë¬¸ì„œëŠ” êµìœ¡ë¶€ì—ì„œ ë°°í¬í•œ ê³µê°œëœ 'ê¸°ì¬ìš”ë ¹ ê°€ì´ë“œë¼ì¸'ì´ë©°, í¬í•¨ëœ ëª¨ë“  ì´ë¦„ê³¼ ì •ë³´ëŠ” **ì„¤ëª…ì„ ìœ„í•´ ë§Œë“¤ì–´ì§„ ê°€ìƒì˜ ì˜ˆì‹œ(Fictional Examples)**ì…ë‹ˆë‹¤. ì‹¤ì œ ê°œì¸ì •ë³´ê°€ ì•„ë‹ˆë¯€ë¡œ ì•ˆì‹¬í•˜ê³  ë¶„ì„í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”."
                ),
                contents=[uploaded_file],
                ttl=datetime.timedelta(minutes=60)
            )

            # (5) ì„¸ì…˜ì— ì •ë³´ ì €ì¥
            st.session_state.cache_name = cache.name
            st.session_state.current_book = selected_file
            st.session_state.messages = [] 
            st.success(f"âœ… ë¶„ì„ ì™„ë£Œ! ì´ì œ ë¹ ë¥´ê³  ì €ë ´í•˜ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.stop()

# 5. ëª¨ë¸ ë¡œë”© ë° ì±„íŒ…
try:
    cached_content = caching.CachedContent.get(st.session_state.cache_name)
    model = genai.GenerativeModel.from_cached_content(cached_content=cached_content)
    
except Exception as e:
    st.error("âš ï¸ ì„¸ì…˜ì´ ë§Œë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (1ì‹œê°„ ê²½ê³¼) ìƒˆë¡œê³ ì¹¨ í•´ì£¼ì„¸ìš”.")
    st.session_state.cache_name = None
    st.stop()

# 6. ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if user_input := st.chat_input("ì§ˆë¬¸í•´ ì£¼ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        try:
            # ì±„íŒ… ê¸°ë¡ êµ¬ì„±
            chat_history = [{"role": m["role"], "parts": [m["content"]]} for m in st.session_state.messages]
            
            response = model.generate_content(chat_history)
            
            # [ìˆ˜ì •ë¨] ë‹µë³€ ì°¨ë‹¨(Safety Block) ì‹œ ì•±ì´ ì£½ì§€ ì•Šë„ë¡ ë°©ì–´ ì½”ë“œ ì¶”ê°€
            if response.parts:
                full_response = response.text
                message_placeholder.markdown(full_response)
                st.session_state.messages.append({"role": "model", "content": full_response})
            else:
                # ë‹µë³€ì´ ì°¨ë‹¨ëœ ê²½ìš°
                error_msg = f"âš ï¸ ë‹µë³€ì´ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\nì°¨ë‹¨ ì‚¬ìœ : {response.prompt_feedback.block_reason}"
                message_placeholder.error(error_msg)
                print(response.prompt_feedback) # ì„œë²„ ë¡œê·¸ì— ìƒì„¸ ë‚´ìš© ì¶œë ¥
                
                # ì°¨ë‹¨ëœ ê²½ìš°ì—ë„ íˆìŠ¤í† ë¦¬ì— ë‚¨ê¸¸ì§€, ì•„ë‹ˆë©´ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì·¨ì†Œí• ì§€ ê²°ì •
                # ì—¬ê¸°ì„œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì„ popí•˜ì—¬ ëŒ€í™”ë¥¼ ë‹¤ì‹œ ì‹œë„í•  ìˆ˜ ìˆê²Œ í•¨
                if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
                    st.session_state.messages.pop()

        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            if st.session_state.messages and st.session_state.messages[-1]["role"] == "user":
                st.session_state.messages.pop()
